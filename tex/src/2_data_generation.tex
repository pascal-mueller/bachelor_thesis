%!TEX root = ../thesis.tex

\section{Data Generation}
% Give quick intro into what kind of data is needed for a DL approach.
In deep learning, one basically tries to fit a mathematical model, the neural
network, to a data set consisting of measurements. This process is called
"learning".
Those measurements might be data from a physical experiment, time series from
the stock market, health information collected for mice or virtually and
other source of data.

In most cases, the data is split into three sets. The first subset is used to
train the neural network and is called training set. Training is repeated several
times and each iteration is called an epoch. The second subset is used to check
the learning progress in each epoch and is called validation set. Once the NN
reached a stated at which we think it can make good prediction we check it by
applying it to the test set for one time. Since the test set wasn't involved in
the learning process at all, it gives us an unbiased view.

In the case of GW we only have a very limited data set of measurements. We can't
make an experiment to generate more measurements, we can only try to detect the
ones which occure naturally. This rises the problem of needing to know how a
gravitational wave looks like to be able to measure it. We solve this by
numerically solving the linearized equations i.e. we simulate it. In this thesis
, simulated gaussian noise as well as simulated signal will be used.

Such a simulated measurement, from now on called sample, either consists of pure
noise or a signal injected into noise. pyCBC resp. lalsuite will be used to 
generate signals utilizing the XXX approximant. 

% Give quick overview about how our data will look like: 1s samples for
% training and validation but 1 month data as test data.
In this thesis we will follow the data generation of [ml strategies] which also
takes into account the problems stated in [29]. The samples in our training and
validation set will be sampled at 2048 Hz with a duration of 1s i.e. each sample
consists of 2048 32 bit values. To address the problems stated in [29], our test
data will be 1 month of data sampled at 2048 Hz.

In this thesis we will only use H1.

\subsection{Training and Validation Sets}
Since detector data
Out NN will take an input of 1 second and a sample rate of 2048 Hz is used. 
Each element of the training and validation set, called a sample, consists of
either pure gaussian noise or a signal injected into noise.

First 400k noise samples are generated. Secondly 200k signal samples are
generated using parameters drawn randomly from TABLE. After that we inject the
200k signals into 200k of the 400k noise samples. This results in a data set
containing 200k pure noise samples as well as 200k noise+signal samples.

Each signal is SNR scaled to 7. This is because the paper said that's good.


First we generate signals. Then we generate noise. Then we inject the signals
into the noise. We end up with samples of either pure noise or signal+noise.

We shuffle the set and make a 80/20 split into training and validation set.

The whole thing is whitened, signals are snr scaled to 7 cause blabla.:q


\subsection{Test Set}
The test set consists of 1 month of data. In this work, we use the ml challenge
generation script with the first dataset. It has some properties yada yada yada.

\subsection{Implementation}

\begin{itemize}
  \item Overview: What and why do we want to generate? Very generally explain
        that there is noise and signal.
  \item SNR: Explain SNR
  \item Properties: Describe proerties like SNR, sample rate, duration.
  \item Training \& Evaluation Data
  \item Test Data
  \item Implementation
\end{itemize}
