%!TEX root = ../thesis.tex

\section{Deep Learning Approach}
% Intro: Explain basic workflow of DL. Introduce all the little parts.
% Explain:
% - Loss function
% - Optimizer (basically just stated parameters)
% - Efficiency
% - Accuracy
% - Neural Network used
% - Training strategy used
% - USR

% Quick introduction to deep learning
Deep learning is a machine learning approach which consists of processing
units, so called \textit{neurons}, which are arranged in an array. Such an array 
makes up a \textit{layer}. One to several such layers make up a
\textit{neural network} (NN). Each neuron
acts like a filter extracting higher-level features from the raw input. (ref)

\autoref{fig:3_main_workflow} depicts a high level view of the deep learning approach workflow.
It starts by reading the $200'000$ samples which were generated as described in
the previous chapter. The data is split whereas 80\% will make up the
training set and 20\% will make up the validation set.
After that, we iterate over the training and validation set for 200 
\textit{epochs}. 

In each epoch we have a training part and a validation part. In each
part we apply the neural network to the training resp. validation data and
compute the respective loss. Computing the training loss is followed by the
usual back propagation and an optimization step. Both are needed for the actual
learning progress. After compution the validation loss in the validation part,
we compute two metrics: The accuracy and the efficiency. Both metrics help
to understand the progress our neural network makes.

\input{../tikz/dl_flowchart.tex}

\newpage

\subsection{The Neural Network}
The neural network used in this work was taken from \cite{PhysRevD.105.043002}
and is described in
%\autoref{table:3_neural_network}.
It consists of 6 convolutional layers followed by two linear layers which each include
dropouts as regularization. While \cite{PhysRevD.105.043002} randomizes the 
initial weights, this isn't done in this work because for once, we aren't
interested in averaging several runs like they authors of \cite{PhysRevD.105.043002}
are but also because the NN always converged fast without utilizing randomized
initial weights.


% Introduction to DL
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\begin{tabular}{|>{\columncolor[HTML]{DAE8FC}}c |ccc|}
\hline
Layer Type                                                    & \multicolumn{3}{c|}{\cellcolor[HTML]{DAE8FC}Parameter}                                                                                                         \\ \hline
                                                              & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}$C_\text{in}$} & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}$C_\text{out}$} & \cellcolor[HTML]{DAE8FC}Kernel Size \\ \hline
Batch Normalization                                           & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                       & \multicolumn{1}{l|}{}               \\ \hline
Conv1d + ELU                                                  & \multicolumn{1}{c|}{1}                                     & \multicolumn{1}{c|}{8}                                      & 64                                  \\ \hline
Conv1d                                                        & \multicolumn{1}{c|}{8}                                     & \multicolumn{1}{c|}{8}                                      & 32                                  \\ \hline
Maxpool1d + ELU                                               & \multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{}                                       & 4                                   \\ \hline
Conv1d + ELU                                                  & \multicolumn{1}{c|}{8}                                     & \multicolumn{1}{c|}{16}                                     & 32                                  \\ \hline
Conv1d                                                        & \multicolumn{1}{c|}{16}                                    & \multicolumn{1}{c|}{16}                                     & 16                                  \\ \hline
Maxpool1d + ELU                                               & \multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{}                                       & 3                                   \\ \hline
Conv1d + ELU                                                  & \multicolumn{1}{c|}{16}                                    & \multicolumn{1}{c|}{32}                                     & 16                                  \\ \hline
Conv1d                                                        & \multicolumn{1}{c|}{32}                                    & \multicolumn{1}{c|}{32}                                     & 16                                  \\ \hline
Maxpool1d + ELU                                               & \multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{}                                       & 2                                   \\ \hline
Flatten                                                       & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                       & \multicolumn{1}{l|}{}               \\ \hline
                                                              & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}in\_features}  & \multicolumn{1}{c|}{\cellcolor[HTML]{DAE8FC}out\_features}  & \cellcolor[HTML]{DAE8FC}p           \\ \hline
Linear                                                        & \multicolumn{1}{c|}{1856}                                  & \multicolumn{1}{c|}{64}                                     &                                     \\ \hline
Dropout + ELU                                                 & \multicolumn{1}{c|}{}                                      & \multicolumn{1}{c|}{}                                       & 0.8                                 \\ \hline
Linear                                                        & \multicolumn{1}{c|}{64}                                    & \multicolumn{1}{c|}{64}                                     &                                     \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{DAE8FC}Dropout + ELU}   & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                       & 0.8                                 \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{DAE8FC}Softmax and USR} & \multicolumn{1}{c|}{64}                                    & \multicolumn{1}{c|}{2}                                      &                                     \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{DAE8FC}}                & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                       & \multicolumn{1}{l|}{}               \\ \hline
\end{tabular}
\end{table}


Note that the output layer consists of a softmax and a linear layer called USR,
which mean that we feed the logits\footnote{The raw output of a layer is called
  logits.} from the last hidden layer to both of them
and return two predictions. The reason we return both is for simplicity only. We
never use both simultaneously.

A softmax layer fullfils the mathematical properties of a probability, which is
why we call its output \textit{pscore}. Note that the pscore does not represent
a statistical significant value but only tells us how confident the network is,
that the sample contains a signal. \cite{PhysRevD.100.063015}

The softmax layer is generally given by \cite{PhysRevD.105.043002}

\begin{equation}\label{softmax}
  \text{Softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_{j=0}^N \exp(x_j)}
\end{equation}

where $\mathbf{x} = (x_0, x_1, \dots, x_N)$ represents the outputs of the previous
layer. $N+1$ is the amount of neurons in the layer, in our case 2. Because physical instruments
have a limited sensitivity, we generally operate with single presicion (32-bit).
Because floating point numbers are an approximation, they introduce an error.
Due to this error \autoref{softmax} might evaluate to 1, indicating the a
pure noise sample contains a signal while analytically it would never be 1.
\cite{PhysRevD.105.043002} To solve this problem, he authors of
\cite{PhysRevD.105.043002} propose the following output layer which they call
\textit{Unbounded Softmax Replacement} \cite{PhysRevD.105.043002}

\begin{equation}\label{usr}
  \text{USR}(\mathbf{x}) = x_0 - x_1
\end{equation}

To understand \autoref{usr} we first recast \autoref{softmax}, for $N=1$, into
a form in which only one term contains the variables $x_0$ and $x_1$

\begin{equation}\label{usr2}
  \frac{\exp(x_0}{\exp(x_0)+\exp(x_1)} = \frac{1}{1 + \exp{x_1 - x_0}}
\end{equation}

Since \autoref{usr2} is bijective, a threshold applied to \autoref{softmax} is
in direct relation with a threshold applied to $x_1 - x_0$.
\cite{PhysRevD.105.043002}

Finally they \cite{PhysRevD.105.043002} argue that they use $x_0 - x_1$ because
$x_0 - x_1 > \hat{x}_0 - \hat{x}_1 \Leftrightarrow \text{Softmax}(\mathbf{x})_0
> \text{Softmax}(\mathbf{\hat{x}})_0$.

Note that subtracting two floating point numbers will still produce an error but
because $x_0 - x_1$ is unbounded, the error is neglectible.


\subsection{Loss Function}
The loss function was chosen as a variant of the binary cross-entropy that is
designed to stay finite \cite{PhysRevD.105.043002}. 

\begin{equation}
  L(\mathbf{y}_t, \mathbf{y}_p) = -\frac{1}{N_b} \sum_{i=1}^{N_b}
  \mathbf{y}_{t,i} \cdot \log(\epsilon + (1 - 2\epsilon) \mathbf{y}_{p,i})
\end{equation}

whereas $\mathbf{y}_p = (1, 0)^T$ if the sample contained a signal or
$\mathbf{y}_p = (0, 1)^T$ if the sample contained pure noise. $\mathbf{y}_p$ is
the prediction of the network for the sample. $N_b = 32$ is the batch size and
$\epsilon = 1e-8$.

The implementation of this loss function can be found in
\hlc{src/reg\_BCELoss.py} and was taken from \cite{MLGWSC1} .

\subsection{Accuracy \& Efficiency}
Accuracy  basically the fraction of correctly classified samples. While
accuracy can't be used to predict how good it will work on the test set
\cite{PhysRevD.100.063015}, it does
give us insight about how good the network is learning the training set. The
reason we can't use accuracy to gain insight about the performance of the NN
on the test set is because the test set is continuous while accuracy is used
for binary data sets where each class is distributed similarly.

As stated in \cite{PhysRevD.105.043002} a better metric to use is the so called
efficiency. The efficiency is given by:

\begin{equation}
  \text{efficiency} = \frac{N_{s>t}}{N_s}
\end{equation}

whereas $N_{s>t}$ is the number of signals with a p-score bigger than the
threshold and $N_s$ is the total number of signals.

The threshold is given by the x-th largest p-score whereas:

\begin{equation}
  x = \lfloor N_n \cdot \text{FAP} \rfloor
\end{equation}

with $\text{FAP} = 0.0001$ and $N_n$ being the total number of samples.

Another reason the efficiency is computed is to be able to comapre it to
\cite{PhysRevD.105.043002}. We expect it to be similar.

\subsection{Sensitivity \& FAR}
To be able to judge our NN on our test data we utilize the
\textit{(luminous) sensitivity distance} as well as the FAR.

The FAR is given by \cite{PhysRevD.105.043002}

\begin{equation}
  \text{FAR} = \frac{N_f}{T_0}
\end{equation}

where $N_f$ is the amount of false positives and $T_0$ is the duration of the
test data. The FAR tells us how many false positives we have per unit time.

The sensitivity distance is given as the radius of the sensitive volume of
the search, which is given by \cite{PhysRevD.105.043002}

\begin{equation}
  \text{V(FAR)} = \text{V}(d_{\text{max}})\frac{N_t(\text{FAR})}{N_i}
\end{equation}

whereas $d_max$ is the maximal distances at which sources are injected.
$\text{V}(d_{\text{max}})$ describes the volume of a sphere and $N_i$ are the
total number of injections. $N_t$ is the number of true positives at a given
FAR.

\begin{itemize}
  \item Problem discrete data: Paper 48
  \item Standardized metrics (FAR, sensitivity): Paper 26
  \item Explain state of the art needs 1 in 2 months, far of 10-4 etc.
\end{itemize}
Paper 48 says that we can't use FAP etc.

Paper 26 standardizes the metrics: FAR and sensitivity

TODO:
\begin{itemize}
  \item Improve NN table
  \item Add references
\end{itemize}
